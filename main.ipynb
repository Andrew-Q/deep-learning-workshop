{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep learning on handwritten digits\n",
    "\n",
    "The MNIST (mixed National Institute of Standards and Technology) dataset ([https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database)) is a classic data set in machine learning. To develop our intuitions about the problem, we start with a simple linear classifier and achieve an average accuracy of $80\\%$. We then proceed to build a state-of-the-art convolutional neural network (CNN) and achieve an accuracy of over $98\\%$.\n",
    "\n",
    "This notebook is available on [https://github.com/jcboyd/deep-learning-workshop](https://github.com/jcboyd/deep-learning-workshop).\n",
    "\n",
    "A Docker image for this project is available on Docker hub:\n",
    "\n",
    "> $ docker pull jcboyd/deep-learning-workshop/:[cpu|gpu]\n",
    "\n",
    "> $ nvidia-docker run -it -p 8888:8888 jcboyd/deep-learning-workshop/:[cpu|gpu]\n",
    "\n",
    "## 1. Machine Learning\n",
    "\n",
    "* Machine learning involves algorithms that find patterns in data.\n",
    "\n",
    "\n",
    "* This amounts to a form of *inductive reasoning*: inferring general rules from examples, with the view of reapplying them to new examples. *Learning by example*\n",
    "\n",
    "\n",
    "\n",
    "* This symbols are labeled (yes/no) $\\implies$ *supervised* learning problem\n",
    "\n",
    "\n",
    "*Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012. (Figure 1.1)*\n",
    "\n",
    "![img/induction.png](img/induction.png)\n",
    "\n",
    "* The corresponding dataset would look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "read_csv(open('data/shapes.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 Classifiers\n",
    "\n",
    "* The above is an example of a *classification* problem. \n",
    "\n",
    "\n",
    "* Each *observation* $\\mathbf{x}$ is represented by a vector of $D$ *features* and has label $y_i$ denoting its class (e.g. yes or no).\n",
    "\n",
    "\n",
    "* Thus, our dataset of $N$ observations is,\n",
    "\n",
    "$$\\mathcal{D} = \\{\\mathbf{x}_i, y_i\\}_{i=1}^N,$$\n",
    "$$\\mathbf{x}_i \\in \\mathbb{R}^D, y_i \\in \\{1, \\dots, C\\}$$\n",
    "\n",
    "\n",
    "* The model will attempt to divide the feature space such that the classes are as separate as possible, creating a *decision boundary*.\n",
    "\n",
    "![img/separation.png](img/separation.png)\n",
    "\n",
    "### 1.2 Data Exploration\n",
    "\n",
    "* The MNIST dataset ([https://en.wikipedia.org/wiki/MNIST_database](https://en.wikipedia.org/wiki/MNIST_database)) is a classic dataset in machine learning.\n",
    "\n",
    "\n",
    "* Derived from a dataset of handwritten characters ``crowdsourced'' from US high school students.\n",
    "\n",
    "\n",
    "* It consists of 60,000 labelled images of handwritten digits 0-9, and a test set of a further 10,000 images.\n",
    "\n",
    "\n",
    "* Notably used as a benchmark in the development of convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from src import data_utilsz\n",
    "\n",
    "Xtr, Ytr, Xte, Yte = data_utils.get_all_data()\n",
    "\n",
    "print 'Training data shape: ', Xtr.shape\n",
    "print 'Training labels shape: ', Ytr.shape\n",
    "print 'Test data shape: ', Xte.shape\n",
    "print 'Test labels shape: ', Yte.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can visualise our data samples with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')\n",
    "from src import vis_utils\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "idx = np.random.randint(len(Xtr))\n",
    "vis_utils.plot_image(ax, Xtr[idx][0:,:,0], Ytr[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* As far as our model will be concerned, the images in the dataset are just vectors of numbers\n",
    "\n",
    "\n",
    "* From this perspective, there is no structural difference between the MNIST problem, and the symbols problem above\n",
    "\n",
    "\n",
    "* The observations just live in 784-dimensional space (28 x 28 pixels) rather than 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print Xtr[idx].reshape((1, 784))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "vis_utils.plot_array(fig, Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3 Data Preprocessing\n",
    "\n",
    "* To help with model assessment, we set aside some *validation* data from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First, vectorise image data\n",
    "Xtr_rows = np.reshape(Xtr, (Xtr.shape[0], -1))\n",
    "Xte_rows = np.reshape(Xte, (Xte.shape[0], -1))\n",
    "\n",
    "# Subsample the data for more efficient code execution in this exercise.\n",
    "num_training = 59000\n",
    "num_validation = 1000\n",
    "num_test = 10000\n",
    "\n",
    "# Create validation split\n",
    "Xtr_rows, Ytr, Xval_rows, Yval, Xte_rows, Yte = data_utils.create_data_splits(\n",
    "    Xtr_rows, Ytr, Xte_rows, Yte, num_training, num_validation, num_test)\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print 'Training data shape: ', Xtr_rows.shape\n",
    "print 'Validation data shape: ', Xval_rows.shape\n",
    "print 'Test data shape: ', Xte_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A typical procedure prior to training is to normalise the data.\n",
    "\n",
    "\n",
    "* Here we subtract the *mean image*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mean_image = np.mean(Xtr, axis=0).reshape(1, 784)\n",
    "\n",
    "Xtr_rows -= mean_image\n",
    "Xval_rows -= mean_image\n",
    "Xte_rows -= mean_image\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, mean_image.reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2 Linear Classification\n",
    "\n",
    "* First we will assume a linear *score function*, that is, a prediction that is a linear combination of inputs and model *weights*,\n",
    "\n",
    "$$f(\\mathbf{x} ; \\mathbf{w}) = \\mathbf{w}^T\\mathbf{x} = w_1x_1 + w_2x_2 + \\dots + w_Dx_D$$\n",
    "\n",
    "\n",
    "* For MNIST, $D = 784$, and we need a weight for every pixel in an image.\n",
    "\n",
    "\n",
    "* The choice of model weights will be *inferred* from the data in a procedure called *training*.\n",
    "\n",
    "\n",
    "*Stanford Computer Vision course--Convolutional Neural Networks for Visual Recognition [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/) *\n",
    "\n",
    "<div style=\"text-align:center\"><img src =\"img/linear.png\"/></div>\n",
    "\n",
    "### 1.3 Model training\n",
    "\n",
    "* Training involves computing a mathematical function that differentiates between observations from different classes--classifier.\n",
    "\n",
    "\n",
    "* We first decide on a form for the function $f$ to take, then we optimise its parameters $\\mathbf{w}$ over the dataset and a loss function, $\\mathcal{L}$,\n",
    "\n",
    "$$\\mathbf{w}^* = \\min_{\\mathbf{w}} \\sum_{i=1}^N \\mathcal{L}(f(\\mathbf{x}_i ; \\mathbf{w}), y_i)$$\n",
    "\n",
    "\n",
    "* The loss function measures how close the classification $f(\\mathbf{x}_i ; \\mathbf{w})$ of observations $\\mathbf{x}_i$ is to the true value $y_i$.\n",
    "\n",
    "\n",
    "* Training consists of finding the weights that minimise the loss over the training set.\n",
    "\n",
    "* The most common procedure for optimising a convex differentiable function is known as *gradient descent*,\n",
    "\n",
    "$$\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\alpha\\nabla\\mathcal{L}(\\mathbf{w}^{(k)})$$\n",
    "\n",
    "where $\\alpha$ is referred to as the step size or *learning rate*. Thus, each iteration is a descent step, and we converge iteratively to a global minimum.\n",
    "\n",
    "![img/gradientdescent.png](img/gradientdescent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from src.linear_models import MultiSVM, SoftmaxRegression\n",
    "\n",
    "# Perform bias trick\n",
    "Xtr_rows = np.append(Xtr_rows, np.ones((Xtr_rows.shape[0], 1)), axis=1)\n",
    "Xval_rows = np.append(Xval_rows, np.ones((Xval_rows.shape[0], 1)), axis=1)\n",
    "Xte_rows = np.append(Xte_rows, np.ones((Xte_rows.shape[0], 1)), axis=1)\n",
    "\n",
    "reg = 5e4\n",
    "batch_size = 200\n",
    "max_iters = 1500\n",
    "learning_rate = 1e-7\n",
    "\n",
    "model = MultiSVM(Xtr_rows, Ytr)\n",
    "model.train(reg, batch_size, learning_rate, max_iters, Xval_rows, Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_test = Yte.shape[0]\n",
    "predictions = [model.predict(Xte_rows[i]) for i in range(num_test)]\n",
    "print 'Error: %.02f%%' % (100 * (1 - float(sum(Yte == np.array(predictions))) / num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from src import vis_utils\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), np.int32)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    confusion_matrix[Yte[i]][predictions[i]] += 1\n",
    "\n",
    "vis_utils.plot_confusion_matrix(ax, confusion_matrix, classes, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look at some of the model's mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "false = np.where(np.not_equal(Yte, predictions))[0]\n",
    "idx = np.random.choice(false)\n",
    "print 'Prediction: %d\\nTrue class: %d' % (predictions[idx], Yte[idx])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, Xte[idx][0:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In pixel-space, the model had moderate success in separating the image clusters.\n",
    "\n",
    "![img/normals.png](img/normals.png)\n",
    "\n",
    "* The optimised weights are those generalising maximally over each of the class observations\n",
    "\n",
    "\n",
    "* We can take each of the weight vectors and plot them as an image to visualise the template they have learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "vis_utils.plot_weights(fig, weights = model.W[:-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep Learning\n",
    "\n",
    "### 2.1 Features\n",
    "* Features provide a *representation* of the objects we want to classify\n",
    "\n",
    "\n",
    "* Designing features is arguably the most difficult and most important aspect of machine learning\n",
    "\n",
    "\n",
    "* Previously we were operating purely on pixel features\n",
    "\n",
    "\n",
    "* One way we might improve is with *feature engineering* (expert knowledge), *feature extraction*\n",
    "(conventional techniques), *feature selection*, or *dimensionality reduction*.\n",
    "\n",
    "\n",
    "* Another approach is to create non-linear transformations based on a *kernel function* (see kernel methods).\n",
    "\n",
    "![img/convolve.png](img/convolve.png)\n",
    "\n",
    "![img/convolution.jpg](img/convolution.jpg)\n",
    "\n",
    "* Another approach is to build the feature learning into the model itself. This is the essence of *representation* or *deep learning*.\n",
    "\n",
    "\n",
    "* Deep learning is characterised by the modeling of a hierarchy of abstraction in the input data\n",
    "\n",
    "\n",
    "* Each layer of a CNN models features as agglomerations of the features in the previous layer (edges to blobs, to anatomical units, to objects, etc.)\n",
    "\n",
    "\n",
    "* Neural networks are particularly amenable hierarchical learning, as hidden layers are easily stacked.\n",
    "\n",
    "\n",
    "* Have surmounted long-standing challenges in artificial intelligence (e.g. AlphaGo).\n",
    "\n",
    "*LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" Nature 521.7553 (2015): 436-444.*\n",
    "\n",
    "*For a live demo of this, see the Deep Visualisation Toolbox (https://www.youtube.com/watch?v=AgkfIQ4IGaM)*\n",
    "\n",
    "### 2.2 Artificial Neural Networks\n",
    "\n",
    "* Neural networks hidden model layers between input and ouptut, passing through non-linear activation function.\n",
    "\n",
    "\n",
    "* (Loosely) inspired by the interaction of neurons in the human brain.\n",
    "\n",
    "\n",
    "* No longer a linear model, outputs are not linear combinations of inputs and model parameters.\n",
    "\n",
    "\n",
    "* Pros (+): Greater flexibility (universal approximator), built-in feature extraction\n",
    "\n",
    "\n",
    "* Cons (-): Harder to train (not convex), theory relatively underdeveloped\n",
    "\n",
    "![img/layers.png](img/layers.png)\n",
    "\n",
    "### 2.3 Convolutional Neural Networks\n",
    "\n",
    "* Convolutional Neural Networks (CNN) are a type of feed-forward neural network wired so as to perform convolutions (image processing) on input data.\n",
    "\n",
    "\n",
    "* As such, feature extraction is built into the classifier, an optimised wrt the same loss function. *Representation learning*.\n",
    "\n",
    "\n",
    "* LeNet is the original network architecture of CNNs, introduced by Yann Lecun in the 90s.\n",
    "\n",
    "\n",
    "* Covolutional layers (loosely) inspired by the human visual cortex\n",
    "\n",
    "\n",
    "*LeCun, Yann, et al. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE 86.11 (1998): 2278-2324.*\n",
    "\n",
    "![img/lenet.png](img/lenet.png)\n",
    "\n",
    "* CNNs were the breakout success in 2012 that won the ImageNet image classification challenge.\n",
    "\n",
    "\n",
    "* *AlexNet* was a *deep* architecture with many convolutional layers.\n",
    "\n",
    "\n",
    "* The result was quickly followed by a paradigm shift in computer vision research, supplanting tailored feature extraction and reinstating neural networks.\n",
    "\n",
    "*Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012.*\n",
    "\n",
    "### 2.4 Tensor Flow\n",
    "\n",
    "* Python machine learning framework developed by Google Brain (deep learning team)\n",
    "\n",
    "\n",
    "* Originally proprietary, made open source (Apache 2.0) in late 2015\n",
    "\n",
    "\n",
    "* Model is built into a *computational graph*\n",
    "\n",
    "\n",
    "* *Tensors* (multi-dimensional arrays of data) *flow* through the computational graph\n",
    "\n",
    "*CNN code adapted from demo code in the official TensorFlow Docker image*\n",
    "\n",
    "![img/tensorflowlogo.png](img/tensorflowlogo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.cnn import ConvolutionalNeuralNetwork\n",
    "model = ConvolutionalNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img/graph.png](img/graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use unflattened data\n",
    "Xval = Xtr[num_training:,:]\n",
    "Xtr = Xtr[:num_training,:]\n",
    "\n",
    "model.train(Xtr, Ytr, Xval, Yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = model.test_model(Xte)\n",
    "correct = np.sum(predictions == Yte)\n",
    "total = predictions.shape[0]\n",
    "\n",
    "print 'Test error: %.02f%%' % (100 * (1 - float(correct) / float(total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vis_utils import plot_confusion_matrix\n",
    "\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "confusion_matrix = np.zeros((num_classes, num_classes), np.int32)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    confusion_matrix[Yte[i]][predictions[i]] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plot_confusion_matrix(ax, confusion_matrix, classes, fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Current world record: 0.21% error from *ensemble* of 5 CNNs with data augmentation\n",
    "\n",
    "*Romanuke, Vadim. \"Parallel Computing Center (Khmelnitskiy, Ukraine) represents an ensemble of 5 convolutional neural networks which performs on MNIST at 0.21 percent error rate.\". Retrieved 24 November 2016.\"*\n",
    "\n",
    "* We can plot the activation maps of the following image as it passes through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Xtr[5]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "vis_utils.plot_image(ax, img[0:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, the 32 activations of the first convolutional layer ($28 \\times 28$ px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_maps = model.getActivations(model.conv1, img.reshape(1, 28, 28, 1))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "vis_utils.plot_activation_maps(fig, activation_maps, 4, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the 64 activations of the second convolutional layer ($14 \\times 14$ px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_maps = model.getActivations(model.conv2, img.reshape(1, 28, 28, 1))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "vis_utils.plot_activation_maps(fig, activation_maps, 8, 8)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
